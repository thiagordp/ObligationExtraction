# Obligation Extraction Project: AI Act study case

This project is designed for extracting and validating deontic (obligation-related) information from legal texts, such as the AI Act provisions. It leverages a structured set of scripts and data for managing, extracting, and validating legal obligations.

## Project Structure

The project structure is organized as follows:

```bash
.
├── data
│   ├── outputs                      # Contains output files generated by extraction scripts
│   │   └── token_distribution_sentences.pdf  # PDF report on token distribution across sentences
│   ├── prompts                      # Contains prompt text files used in LLM interactions
│   │   ├── system_prompt.txt        # Prompt used by the system for task setup
│   │   └── user_prompt.txt          # Prompt customized for the user
│   ├── raw                          # Contains raw input data files
│   │   ├── ai_act_provisions.json   # JSON file with provisions from the AI Act
│   │   ├── ignored_provisions.json  # JSON file specifying provisions to be ignored
│   │   ├── sample_template.txt      # Template for data sample structuring
│   │   └── selected_provisions.json # JSON file with selected provisions for analysis
│   └── validation                   # Directory for validation-related data (currently empty)
├── examples                         # Directory containing example outputs
│   └── output_example.txt           # Example output file demonstrating expected output format
├── logs                             # Directory for storing log files from script runs
├── run_extraction_validation.py     # Script to run extraction and validation processes
├── run_obligation_extraction.py     # Script for running the obligation extraction process
└── src                              # Source code directory for core extraction and validation modules
    ├── deontic_extraction.py        # Core module for obligation extraction logic
    ├── extraction_validation.py     # Module for validating extraction results
    └── llm.py                       # Module managing interactions with the language model
```

## Setup and Dependencies

1. **Environment Variables**: Environment variables are loaded using `.env` files (make sure to set this up as needed).
2. **Dependencies**: Install required dependencies (if not using a specific environment file):
   ```bash
   pip install -r requirements.txt
   ```

## Usage

### Running Obligation Extraction

Use `run_obligation_extraction.py` to extract obligations from the provided data.

```bash
python run_obligation_extraction.py
```

This script:
1. Loads environment configurations.
2. Sets up logging.
3. Organizes filtered data for analysis.
4. Extracts obligations from the data using the `obligation_extraction()` function.
5. Divides sample data for further analysis.

### Running Extraction Validation

To validate the extraction results, use `run_extraction_validation.py`:

```bash
python run_extraction_validation.py
```

This script:
1. Calculates validation metrics using `calculate_validation_metrics()` from `extraction_validation.py`.
2. Produces statistical summaries of the extracted data.

## Core Modules

- **deontic_extraction.py**: Contains functions for deontic obligation extraction:
  - `divide_sample`: Divides sample data for analysis.
  - `obligation_extraction`: Main function to extract obligations.
  - `organize_filtered_data`: Organizes data for easier extraction.
  - `setup_logging`: Initializes logging for tracking progress and errors.

- **extraction_validation.py**: Contains functions for validating the accuracy and quality of extraction:
  - `calculate_validation_metrics`: Calculates accuracy and consistency metrics for validation.
  - `calculate_statistics`: Computes statistical summaries of the extraction results.

- **llm.py**: Manages the interaction with a language model, facilitating the extraction and validation of deontic statements.

## Data

The data files are organized into various categories:
- **Prompts** (`data/prompts`): Contains prompt files used in language model interactions.
- **Raw Data** (`data/raw`): Contains raw JSON and template files for processing.
- **Outputs** (`data/outputs`): Stores generated output files.
- **Validation Data** (`data/validation`): Intended for validation data, currently empty.

## Logging

All logs generated during script executions are saved in the `logs` directory for troubleshooting and process tracking.

